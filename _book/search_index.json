[
["index.html", "Postgraduate Research Methods and Analysis Section 1 Overview 1.1 Aims of the module 1.2 Learning Outcomes 1.3 Indicative module content 1.4 Assessment", " Postgraduate Research Methods and Analysis Christopher J. Wilson 2020-07-22 Section 1 Overview 1.1 Aims of the module The module aims to introduce and develop core and advanced principles of quantitative psychological research design and statistical analysis. Specifically, it will begin with the foundations of designing and conducting empirical research projects and progress to explore a range of statistical analysis techniques and software suited to dealing with different kinds of data. The overall aim is to give students the confidence to develop appropriate research questions, generate testable hypotheses, design rigorous projects, collect data and select appropriate statistical procedures that lead to defensible scientific conclusions. 1.2 Learning Outcomes On successful completion of this module, students will be able to: Personal &amp; Transferable Skills Display advanced competency in the numeric, statistical and analytical skills required for psychological research projects. Demonstrate advanced knowledge and skills in research design. Demonstrate effective ability to conduct and interpret statistical analyses using SPSS. Research Knowledge &amp; Cognitive Skills Interpret and evaluate quantitative methods in psychology. Professional Skills Demonstrate an in-depth knowledge and understanding of research design and applied statistical techniques relevant to a specialist area of psychology. 1.3 Indicative module content This module includes (but is not limited to): Experimental, quasi-experimental and non-experimental research designs Research Philosophy Ethics Probability and Hypothesis Testing Power Moderation and Mediation Using R for data analysis Correlation and Regression ANOVA and Factorial ANOVA Reliability, Validity and Psychometrics 1.4 Assessment 1.4.1 Assessment details ECA (100% of the overall mark): Produce a 3,000 word research report based on data gathered from a battery of psychological assessments to facilitate either a non-experimental or experimental design, including: a literature review, a testable research hypothesis, replicable methodology, appropriate results analysis, discussion of the findings and appropriate limitations and future work. Learning outcomes: 1, 2, 3, 4 and 5. Formative assessment: Formative assessment and feed forward are embedded within the module from an early stage. Verbal feed forward is provided during inquiry and practice based learning activities and assessment preparation sessions. 1.4.2 Specific marking criteria for research report (in addition to generic assessment criteria): Students should produce a detailed and evidence based rationale leading to a testable hypothesis. A methodology detailed enough for successful replication should be provided (including information on design, participants). Data should be analysed correctly via an appropriate method with evidence of correct interpretation and assumption checking. A discussion of the findings which relates results to theory should be presented. Appropriate threats to validity should be evaluated and discussed as limitations and implications for future work. "],
["introduction-to-r-and-r-studio.html", "Section 2 Introduction to R and R Studio 2.1 By the end of this section, you should be able to: 2.2 Why learn / use R? 2.3 R has many advantages 2.4 Download R and R Studio 2.5 The R Studio environment 2.6 Working with a script 2.7 Installing and loading packages", " Section 2 Introduction to R and R Studio 2.1 By the end of this section, you should be able to: Download R and R studio Identify the R script, R console, Data environment and file browser in R studio Write and run R code from a script Install and load R packages 2.2 Why learn / use R? 2.2.1 Some information about R R is developed and used by scientists and researchers around the world Open source = no cost Constant development Connects to other data science/research tools Worldwide community: training widely available Encourages transparency and reproducibility Publication-ready outputs 2.2.2 Moving from other software to R Workflow is different Organise files and data differently Workspace can contain data and outputs Can manage multiple datasets within a workspace Learning curve can be steep initially e.g. Variables and coding, scripts Need to know what you want e.g. building your regression model / ANOVA error terms 2.3 R has many advantages Using scripts means analysis is easy to follow and reproduce R scripts are small, online collaboration, no SPSS “older version” problems Data can be organised and reorganised however you need it (tidyr) Packages are available for “cutting edge” analysis: e.g. Big Data &amp; Machine Learning A robust language for precise plots and graphics (ggplot) R analysis code can be embdeded into documents and presentations (R Markdown) 2.4 Download R and R Studio Click on these links to download: R project RStudio 2.5 The R Studio environment The interface for R Studio looks daunting at first. However, there are 4 main sections, 2 on the left and 2 on the right. MAIN TOP: R Script files or R Document Files Where we usually type our code as a script before we run it. Script files are usually saved so we can work on them and rerun the code again later (.R files). MAIN BOTTOM: Console Shows the output of our R code. We can type R code directly into the console and the answer will ouput immediately. However, it is more convenient to use script files. RIGHT TOP: Environment Contains all of the objects (e.g. data, analysis, equations, plots) that are currently stored in memory. We can save all of this to a file and load it later (.RData files). RIGHT BOTTOM: File Browser The folder that R is working from is called ‘the working directory’ and it will automatically look for files there if we try to import something (e.g. a data file). Using the more button on the file browser allows you to set your desired working directory. 2.6 Working with a script Scripts can be opened from the File menu. Creating a new script The purpose of scripts is to allow you to type your analysis code and save it for use later. Scripts include, for example: Code for importing data into R Your analysis code (e.g. t-test or descriptive statistics) Code for graphs and tables Comments and notes (preceded by the ‘#’ symbol) Example of an R script To run a script, you click the Run button. You can choose to: Run the whole script Run the selected line of code The run button When you run the script, you will normally see output in the console. Output appears in the console If your script contains code for a plot (graph), it will appear in the Plots window in the bottom right. Plots appear in the plot window 2.7 Installing and loading packages install Packages from RStudio, Inc. on Vimeo. Packages add functionality to R and allow us to do new types of analysis. They can be installed via the menu (Tools -&gt; Install Packages) The can also be installed using code: install.packages() For example, TidyR is a package that contains functions for sorting and organising data. To install the package: Installing a package in RStudio or use the code: install.packages(“tidyr”) Once a package is has been installed, you need tp load it using the library() command. For example: library(“tidyr”) "],
["working-with-data-in-r.html", "Section 3 Working with data in R 3.1 By the end of this section, you will be able to: 3.2 In this section, we will use the Tidyverse set of packages 3.3 Import data into R from excel, SPSS and csv files 3.4 Understanding objects in R 3.5 Identify different data structures and variable types 3.6 Working with dataframes", " Section 3 Working with data in R 3.1 By the end of this section, you will be able to: Import data into R from excel, SPSS and csv files Save data to objects Identify different data structures and variable types Convert variables from one type to another Order, filter and group data Summarise data Create new variables from data 3.2 In this section, we will use the Tidyverse set of packages A ‘toolkit’ of packages that are very useful for organsing and manipulating data We will use the haven package to import SPSS files We will use the dplyr to organise data Also includes the ggplot2 and tidyR packages which we will use later To install: install.packages(“tidyverse”) (See the previous section on installing packages) 3.3 Import data into R from excel, SPSS and csv files We can import data from a range of sources using the Import Dataset button in the Environment tab: Importing data It is also possible to import data using code, for example: # importing a .csv file library(readr) studentData &lt;- read_csv(&quot;Datasets/studentData.csv&quot;) #importing an SPSS file library(haven) mySPSSData &lt;- read_sav(&quot;Datasets/salesData.sav&quot;) Once the data are imported, it will be visible in the environment: Imported data in the environment 3.4 Understanding objects in R In R, an object is anything that is saved to memory. For example, we might do some analysis: mean(happiness) However, in the example above, the result would appear in the console but not be saved anywhere. To store the result for reuse later, we save it to an object: happinessMean &lt;- mean(happiness) In the above code (reading left to right): We name the object “happinessMean”. This name can be anything we want. The arrow means that the result of the code on the right will be saved to the object on the left. The code on the right of the arrow calculates the mean of happiness data When this code is run, happinessMean will be stored in the environment window: Result of a calculation in the environment To recall an object from the environment, we can simply type its name. For example: happinessMean Its important to note that anything can be stored as an object in R and recalled later. This includes, dataframes, the results of statistical calculations, plots etc. 3.5 Identify different data structures and variable types 3.5.1 Data structures There are many different types of data that R can work with. The most common type of data for most people tends to be a dataframe. A dataframe is what you might consider a “normal” 2-dimensional dataset, with rows of data and columns of variables: A dataframe example R can also use other data types. A vector is a one-dimensional set of values: # a vector example scores &lt;- c(1,4,6,8,3,4,6,7) A matrix is a multi-dimensional set of values. The below example is a 3-dimensional matrix, there are 2 groups of 2 rows and 3 columns: ## , , 1 ## ## [,1] [,2] [,3] ## [1,] 1 3 5 ## [2,] 2 4 6 ## ## , , 2 ## ## [,1] [,2] [,3] ## [1,] 7 9 11 ## [2,] 8 10 12 We will primarily work with dataframes (and sometimes vectors), as this is how the data in psychology research is usually structured. 3.5.2 Variable types With numerical data, there are 4 key data types: Nominal (a category, group or factor) Ordinal (a ranking) Interval (scale data that can include negative values) Ratio (scale data that cannot include negative values) R can use all of these variable types: Nominal variables are called factors Ordinal variables are called ordered factors Interval and ratio variables are called numeric data and can sometimes be called integers (if they are only whole numbers) or doubles (if they all have decimal points) R can also use other data types such as text (character) data. 3.5.3 Convert variables from one type to another When we first import data into R, it might not recognise the data types correctly. For example, in the below data, we can see the intervention variable : ## participant intervention happiness ## 1 14 2 6.392150 ## 2 18 2 7.119858 ## 3 16 2 7.362572 ## 4 2 2 8.206567 ## 5 4 1 9.565625 ## 6 1 1 9.649888 ## 7 15 1 9.746751 ## 8 5 1 9.783817 ## 9 13 1 9.865528 ## 10 3 2 9.931643 In the intervention variable, the numbers 1 and 2 refer to different intervention groups. Therefore, the variable is a factor variable. To ensure that R understands this, we can resave the intervention variable as a factor using the as.factor() function: happinessSample$intervention &lt;- as.factor(happinessSample$intervention) 3.6 Working with dataframes 3.6.1 Order, filter and group data 3.6.2 Create new variables from data "],
["using-regression-instead-of-anova.html", "Section 4 Using regression instead of ANOVA 4.1 Analysis of covariance", " Section 4 Using regression instead of ANOVA People are often taught to use ANOVA to compare groups (i.e. if you have a categorical IV) and regression if you have continuous IVs. However, ANOVA and regression are the same thing, so it is possible to use regression to do analysis instead of ANOVA or ANCOVA. However, it might be difficult to understand how this is, so let’s look at an example. The dataset Baumann compares 3 different methods of teaching reading comprehension. For this example, we will just look at the variable post.test.1 as the DV. 4.0.1 ANOVA Approach ANOVA asks the question in the following way: Is there a difference in reading comprehension scores between teaching groups? The analysis takes the following approach: What are the means of groups 1,2 and 3? Are the means of groups 1,2 and 3 different? Is the difference in means of groups 1,2 and 3 statistically significant? If we were to summarise the data, we might present it in the following way: ## `summarise()` ungrouping output (override with `.groups` argument) group mean sd Basal 6.681818 2.766920 DRTA 9.772727 2.724349 Strat 7.772727 3.927095 In Table @ref(tab:data_summary) can see that the mean scores are different and highest in the DRTA group. If we were to run an ANOVA on the data, we might present it in the following way: term df sumsq meansq statistic p.value group 2 108.1212 54.06061 5.317437 0.0073468 Residuals 63 640.5000 10.16667 NA NA Notice that the ANOVA (Table @ref(anova_approach)) tells us that the difference between groups is significant (p &lt; 0.05) but we cannot tell yet which of the 3 groups are significantly different from each other. 4.0.2 Regression approach Regression asks the question the following way: Does teaching group predict reading comprehension score? The analysis takes the following approach: Let’s use the mean of group 1 as a reference point (i.e. the intercept). What’s the difference between the intercept and the mean scores of the other groups (i.e. the coefficients)? Are any of the coefficients statistically significant? If we run a regression analysis, we might present the results like this: R2 0.1444271 term df sumsq meansq statistic p.value group 2 108.1212 54.06061 5.317437 0.0073468 Residuals 63 640.5000 10.16667 NA NA term estimate std.error statistic p.value (Intercept) 6.681818 0.6797950 9.829167 0.0000000 groupDRTA 3.090909 0.9613753 3.215091 0.0020583 groupStrat 1.090909 0.9613753 1.134738 0.2607841 If we look at the coefficient (estimate) for the intercept (see Table @ref(tab:regression_approach)), we can see that the value is the same as the mean of the Basal group in the previous section (Table @ref(tab:data_summary)). Furthermore, if we look at the estimates of DRTA and Strat, we can see that the values are the difference between their mean score, and the score for of the intercept (BASAL) group. So the regression table is already comparing the 3 groups. 4.1 Analysis of covariance "],
["probability-and-hypothesis-testing.html", "Section 5 Probability and hypothesis testing 5.1 We collect data from sample but we estimate about the population 5.2 Variance in sample data influences our confidence in population estimates 5.3 We can use confidence intervals to make educated guesses about the population mean 5.4 We can also make confidence intervals of differences between means 5.5 The null hypothesis and statistical significance", " Section 5 Probability and hypothesis testing 5.1 We collect data from sample but we estimate about the population One of the most important things to remember about hypothesis testing in statistics is why we use the approaches we do. That is, we need statistical approaches to test hypotheses because we can only collect data from samples of the population but our research questions and hypotheses apply to whole populations. For that reason, we need a way to estimate how well the sample reflects the population. It is common for us to want to know what the mean (average) response of the population is on certain measures. For example, we might ask the question “what is the average score on this measure of happiness?”. In reality we can only measure a subset (sample) of the population, so we test as many people as we can. Below is a sample of 20 participants: Table 5.1: Some sample data for happiness score participant intervention happiness 1 1 9.649888 2 2 8.206567 3 2 9.931643 4 1 9.565625 5 1 9.783817 6 1 10.610950 7 2 11.543277 8 2 10.055113 9 1 10.780646 10 2 10.877890 11 2 10.025996 12 1 13.106269 13 1 9.865528 14 2 6.392150 15 1 9.746751 16 2 7.362572 17 2 10.711823 18 2 7.119858 19 1 10.044940 20 1 10.952521 5.2 Variance in sample data influences our confidence in population estimates We can see from the table above that the mean of the sample is 9.8166913. However, this is not to say that the population mean is 9.8166913. For one thing, we can see that the range of scores in the sample is between 6.3921503 and 13.1062692. The standard deviation of the sample is 2. The fact that there is so much variance from person to person within our sample indicates that we are likely to be incorrect if we assume that the sample mean is the same as the population mean. The more variance there is within the sample data, the less confident we can be that the sample mean is an accurate representation of the population mean. Another thing that affects our ability to generalise from sample to population is that the sample size is only 20. Larger samples are less influenced by individual outliers, so the larger the sample size is, the more confident we can be that the sample mean is representative of the population mean (provided that the participant sample is representative of the population and recruited in a way to minimise bias). The standard error of the mean can be calculated to estimate how far the mean of the sample data is likely to be from the true population mean. It uses the concepts of variance and sample size to make this estimate. Standard error is calculated by dividing standard deviation by the square root of the sample size (\\(SE = \\frac{SD}{\\sqrt{\\left[n\\right]}}\\)) In R, we can calculate the standard error of the happiness data like so: standardError &lt;- sd(happiness)/sqrt(length(happiness)) ## Calculate standard error standardError #Display the standard error ## [1] 0.3496643 The standard error of our sample mean is 0.35. This suggests that using the sample mean is likely to be 0.35 away from the population mean. 5.3 We can use confidence intervals to make educated guesses about the population mean Using the standard error, we can also create Confidence intervals, which are a range of values, within which the population mean is likely to fall. For example, we know from normal distribution that 95% of the population lies between +/- 1.96 standard deviations of the mean. If we use our sample mean (\\(\\bar{x}\\)) in place of the population mean and include the standard error to account for errors in our estimate, we come up with the following formula for 95% confidence intervals of the mean: Lower confidence interval = \\(\\bar{x} - 1.96*SE\\) Upper confidence interval = \\(\\bar{x} + 1.96*SE\\) mean(happiness) - 1.96 * standardError # Lower confidence interval ## [1] 9.131349 mean(happiness) + 1.96 * standardError # Upper confidence interval ## [1] 10.50203 The value of 1.96 come from the normal distribution, where 95% of the population lies between +/- 1.96 standard deviations of the mean. If we did not already know this, we could use the qnorm() function in R to calculate the value: # Calculate the number of standard deviations that contains 0% to 97.5% of the data (100% - 2.5%). # We can then say that 95% of the data lies between + or - the answer: qnorm(0.975) ## [1] 1.959964 However, with smaller samples, since we are less confident about generalising to the population, we use the t-distribution to calculate that value. The shape of a t-distribution changes based on the sample size, so the smaller the sample size is, the wider the range that 95% of values lie between. We can calculate the 95% value for a particular sample size in R using the qt() function: # The qt function relates to t-distribution qt(0.975,df=20-1) ## [1] 2.093024 # for qt, we need to specify the degress of freedom, which is sample size minus 1 We can see that when we have a sample size of 20, 95% of values in our predicted population distribution will lie between + and - 2.0930241 standard deviations. Therefore, we can calculate more accurate confidence intervals using this value: mean(happiness) - qt(0.975,df=20-1) * standardError # Lower confidence interval ## [1] 9.084835 mean(happiness) + qt(0.975,df=20-1) * standardError # Upper confidence interval ## [1] 10.54855 This tells us: if we were to take infinite number of similar samples, about 95% of their confidence intervals would contain the population mean. Therefore, we think it is reasonable to estimate that the population mean is somewhere in this range. Often people say that a 95% confidence interval means that there is a 95% chance that the population mean is between the lower and upper confidence interval. This is not an accurate statement, but it is often used as a shorthand to help people conceptualise what confidence intervals are. 5.4 We can also make confidence intervals of differences between means Often when we test hypotheses, we are testing the difference between two samples. For example, we might have 2 groups who have undergone different psychological interventions and want to know whether the difference we see our participant samples is likely to generalise to the population. Table 5.2: Some sample data for happiness score with participants divided into 2 groups participant intervention happiness 1 1 9.649888 2 2 8.206567 3 2 9.931643 4 1 9.565625 5 1 9.783817 6 1 10.610950 7 2 11.543277 8 2 10.055113 9 1 10.780646 10 2 10.877890 11 2 10.025996 12 1 13.106269 13 1 9.865528 14 2 6.392150 15 1 9.746751 16 2 7.362572 17 2 10.711823 18 2 7.119858 19 1 10.044940 20 1 10.952521 Using the same approach as in the previous section, we can estimate a confidence interval based on the difference in means and the sample size: # Calculate the number of standard deviations for 95% of the data qt(0.975,df=20-2) # since there are 2 intervention groups, degrees of freedom is now 20-2 ## [1] 2.100922 group1 &lt;- happinessSample %&gt;% filter(intervention ==1) %&gt;% summarise(mean = mean(happiness), sd= sd(happiness)) group2 &lt;- happinessSample %&gt;% filter(intervention ==2) %&gt;% summarise(mean = mean(happiness), sd= sd(happiness)) # calculate mean of difference meanDifference &lt;- group1$mean - group2$mean seDifference &lt;- sqrt(group1$sd^2/19 + group2$sd^2/19) # calculate 95% CI of this meanDifference - seDifference * qt(c(0.975), 20-2) # lower CI ## [1] 0.1794904 meanDifference + seDifference * qt(c(0.975), 20-2) # upper CI ## [1] 2.196518 This tells use that the 95% confidence interval of the difference is between 0.1794904 and 2.1965185. An important part of interpreting this, is to notice whether any point between these values is equal to zero. If the confidence interval of a difference contains a zero value, this means that in future research, with similar samples, it would be possible to see zero difference between the groups. If, on the other hand, the confidence interval does not cross zero, then it is likely that in future research, with similar samples, we would see some difference between the means. The fact of whether confidence intervals cross zero (or not) is linked directly to the idea of hypothesis testing and statistical significance. 5.5 The null hypothesis and statistical significance Using the same study from the previous example: we know that the null hypothesis can be phrased as “in the population, there is no difference between groups”. We then see how the confidence interval of a difference can help us test the null hypothesis: if the null hypothesis were not true, then it is unlikely that the confidence interval of the difference would contain zero. t.test(data= happinessSample, happiness ~ intervention) ## ## Welch Two Sample t-test ## ## data: happiness by intervention ## t = 1.7954, df = 14.658, p-value = 0.09322 ## alternative hypothesis: true difference in means is not equal to 0 ## 95 percent confidence interval: ## -0.2252074 2.6012162 ## sample estimates: ## mean in group 1 mean in group 2 ## 10.410693 9.222689 "]
]
